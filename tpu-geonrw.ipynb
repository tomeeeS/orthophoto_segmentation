{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math, re, os\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"62a94ebf-9c46-43fc-864a-72aeae5a9120","_cell_guid":"2dcb776f-57e6-41bf-b518-f1cfdec78b59","execution":{"iopub.status.busy":"2021-09-14T03:44:23.339906Z","iopub.execute_input":"2021-09-14T03:44:23.340378Z","iopub.status.idle":"2021-09-14T03:44:28.198072Z","shell.execute_reply.started":"2021-09-14T03:44:23.340255Z","shell.execute_reply":"2021-09-14T03:44:28.196701Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tensorflow version 2.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('geonrw-patches') # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\nGCS_DS_PATH","metadata":{"_uuid":"09cd2777-25d2-465f-9ac2-8c8b1a34cb81","_cell_guid":"3d677589-7ad0-46aa-8b8c-6f7371d3151f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T03:44:28.199625Z","iopub.execute_input":"2021-09-14T03:44:28.199956Z","iopub.status.idle":"2021-09-14T03:44:28.712914Z","shell.execute_reply.started":"2021-09-14T03:44:28.199921Z","shell.execute_reply":"2021-09-14T03:44:28.712164Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'gs://kds-2da24647f44cced3d49838d7c8b0c02ed79a33d3ad04ad70aa543eac'"},"metadata":{}}]},{"cell_type":"code","source":"RESUNETA_SAVED_PATH = KaggleDatasets().get_gcs_path('resuneta-trained-p256')\nRESUNETA_SAVED_PATH","metadata":{"execution":{"iopub.status.busy":"2021-09-14T03:44:28.714594Z","iopub.execute_input":"2021-09-14T03:44:28.714917Z","iopub.status.idle":"2021-09-14T03:44:44.633886Z","shell.execute_reply.started":"2021-09-14T03:44:28.714884Z","shell.execute_reply":"2021-09-14T03:44:44.633099Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'gs://kds-0c3997d4b61e0c01305ad79b43d9d5777050be683ba836df79d562f7'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install keras_unet_collection tensorflow-addons","metadata":{"_uuid":"e0fed1be-93ca-44ca-bce7-95b2ee5ecee0","_cell_guid":"600412ef-0ecc-45d0-8a64-a515b2b0b34b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T03:44:44.635439Z","iopub.execute_input":"2021-09-14T03:44:44.635781Z","iopub.status.idle":"2021-09-14T03:44:52.749262Z","shell.execute_reply.started":"2021-09-14T03:44:44.635747Z","shell.execute_reply":"2021-09-14T03:44:52.748270Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting keras_unet_collection\n  Downloading keras_unet_collection-0.1.11-py3-none-any.whl (67 kB)\n\u001b[K     |████████████████████████████████| 67 kB 607 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.12.1)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.12.1)\nInstalling collected packages: keras-unet-collection\nSuccessfully installed keras-unet-collection-0.1.11\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\n\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"_uuid":"cbbaee92-40d4-43f8-b2d4-8af7edf9caaa","_cell_guid":"3da660c8-2cdf-4ac4-892a-4c41095e7c96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T03:44:52.750646Z","iopub.execute_input":"2021-09-14T03:44:52.751013Z","iopub.status.idle":"2021-09-14T03:44:54.406210Z","shell.execute_reply.started":"2021-09-14T03:44:52.750943Z","shell.execute_reply":"2021-09-14T03:44:54.405200Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of accelerators:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# use previous weights and continue from there, or restart and delete previous state\nCONTINUE_LEARNING = True\n\nN_EPOCHS = 250\nUSE_ELEVATION = True\n\nPATCH_SIZE = 256  # should be divisible by 32 (based on the models used)\nTRAIN_RATIO = 0.85  # ratio of inputs used for training, out of all the inputs (=1)\nVALIDATION_RATIO = 0.08\n\nBASIC_BATCH_SIZE = 2\nUNET_BATCH_MULTIPLIER = 4  # U-Net uses less RAM than ResUNet-a, can handle more patches at once\n\n\n# - if we want to hasten the training (or evaluating) process, we can specify a number\n#    to divide the amount of available inputs (for a specific purpose in question) with.\nDATA_DIVIDER = 1.  # this way there will be (16 |) 48 training tfrecords used\n    # (good for TPU). (If TRAIN_RATIO stays 0.8)\n\nUSED_IMG_COUNT = 7356  # out of 7356 images. lower it to save disk space with patch generation\n\nGROUND_TRUTH_SHAPE = [PATCH_SIZE, PATCH_SIZE, 1]\n\nIMAGE_COUNT_PER_RECORD = 123\nif PATCH_SIZE <= 256:\n    PATCH_PER_IMAGE = 16\n    TPU_BATCH_SIZE_MULTIPLIER = 64\nelse:\n    PATCH_PER_IMAGE = 4\n    TPU_BATCH_SIZE_MULTIPLIER = 128\n\nif USE_ELEVATION:\n    N_BANDS = 5  # RGB, alpha (because we use masking), elevation\nelse:\n    N_BANDS = 4\nCLASS_COUNT = 9  # see in predict.py\n\nEARLY_STOPPING_PATIENCE = 24\n\nPATH_TO_DATA = GCS_DS_PATH\nIMGS_DIR = PATH_TO_DATA + \"/images\"\nELEVATION_DIR = PATH_TO_DATA + \"/elevation\"\nMASKS_DIR = PATH_TO_DATA + \"/masks\"\nPATCHES_PATH = PATH_TO_DATA + '/patches/p{}'.format(PATCH_SIZE)\nINPUT_PATH = PATCHES_PATH + '/{}'\nOUTPUT_DIR = \"output\"\nCONFUSION_MATRIX_DIR = OUTPUT_DIR + '/confusion_matrix'\n\nCATEGORIES = {\n    'woodland': 1,\n    'water': 2,\n    'agricultural': 3,\n    'urban': 4,\n    'grassland': 5,\n    'railway': 6,\n    'roads': 7,\n    'buildings': 8,\n}\n\n\n        \n        \n\n\n# data_handling:\nimport math\nimport random\nimport sys\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom numpy import ndarray\nfrom rasterio import DatasetReader\nfrom rasterio.enums import Resampling\nimport tensorflow_addons as tfa\n\nDOWNSCALING_RATE = 1.\n\nORIGINAL_IMAGE_PATCH_KEY = 'original'\nELEVATION_KEY = 'normalized_elevation'\nGT_KEY = 'gt'\nTFREC_FORMAT = {\n    ORIGINAL_IMAGE_PATCH_KEY: tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n    ELEVATION_KEY: tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n    GT_KEY: tf.io.FixedLenFeature([], tf.string),   # shape [] means single element\n}\n\n\n# we can read bigger images one band at a time\ndef read_one_band(\n    rasterio_reader: DatasetReader,\n    band_id,\n    crop_to=sys.maxsize,\n    scale_factor=1. / DOWNSCALING_RATE\n):\n    # resample data to target shape\n    data = rasterio_reader.read(\n        band_id,\n        out_shape=(\n            1,\n            int(rasterio_reader.height * scale_factor),\n            int(rasterio_reader.width * scale_factor)\n        ),\n        resampling=Resampling.mode\n    )\n    return np.array(data[:crop_to, :crop_to])\n\n\ndef from_one_hot_to_category_indices(data: ndarray):\n    \"\"\" converts data (numpy array) that has one-hot encoded categories in its last dimension\n        to category indices (with the other dimensions remaining) \"\"\"\n    return np.argmax(data, axis=-1)\n\n\ndef read_image(img_path):\n    return np.array(cv2.imread(img_path), dtype=np.uint8)\n\n\ndef read_mask(mask_path):\n    return np.array(cv2.imread(mask_path), dtype=np.uint8)[:, :, 0]\n\n\ndef normalize(img):\n    return np.array(img, dtype=np.float32) / 255.\n\n\n# with per-image normalization\ndef read_elevation(elevation_path):\n    e = np.array(Image.open(elevation_path))\n    min_elevation = np.min(e)\n    e = (e - min_elevation) / (np.max(e) - min_elevation)\n    return e\n\n\ndef segmentation_map_from_mask(categories):\n    image = np.zeros((categories.shape[0], categories.shape[1], 3), dtype=np.float32)\n    for i in range(categories.shape[0]):\n        for j in range(categories.shape[1]):\n            for k in range(3):\n                image[i, j, k] = CATEGORY_COLORS[categories[i, j]][k]\n    image = image.transpose([2, 0, 1])\n    return categories, normalize(image)\n\n\ndef add_channel(channel, original_image):\n    return np.append(original_image, [channel], axis=0)\n\n\ndef add_masked_alpha_channel(mask, original_image):\n    alpha = np.zeros(mask.shape, dtype=np.float32)\n    alpha[np.nonzero(mask)] = 1.\n    return add_channel(alpha, original_image)\n\n\ndef add_channel_tf(channel, original_image):\n    return tf.concat([original_image, channel], axis=-1)\n\n\ndef add_masked_alpha_channel_tf(ground_truth, original_image):\n    alpha = tf.where(tf.not_equal(ground_truth, tf.constant(0.)), ones, zeros)\n    return add_channel_tf(alpha, original_image)\n\n\ndef convert_to_example(original_patch, elevation, gt):\n    return tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                ORIGINAL_IMAGE_PATCH_KEY: bytes_feature(tf.io.serialize_tensor(original_patch)),\n                ELEVATION_KEY: bytes_feature(tf.io.serialize_tensor(elevation)),\n                GT_KEY: bytes_feature(tf.io.serialize_tensor(gt))\n            }\n        )\n    )\n\n\ndef parse_example(example_proto):\n    # Parse the input tf.Example proto using the dictionary TFREC_FORMAT\n    features = tf.io.parse_single_example(example_proto, TFREC_FORMAT)\n\n    ground_truth = features[GT_KEY]\n    ground_truth = tf.io.parse_tensor(ground_truth, np.uint8)\n    ground_truth = tf.reshape(ground_truth, GROUND_TRUTH_SHAPE)\n    mask = tf.cast(ground_truth, dtype=tf.float32)\n\n    original_image_patch = features[ORIGINAL_IMAGE_PATCH_KEY]\n    original_image_patch = tf.cast(\n        tf.io.parse_tensor(original_image_patch, np.uint8),\n        dtype=tf.float32)\n\n    elevation_patch = features[ELEVATION_KEY]\n    elevation_patch = tf.io.parse_tensor(elevation_patch, np.float32)\n    elevation_patch = tf.reshape(elevation_patch, GROUND_TRUTH_SHAPE)\n\n    if USE_ELEVATION:\n        original_image_patch = add_channel_tf(elevation_patch, original_image_patch)\n    original_image_patch = add_masked_alpha_channel_tf(mask, original_image_patch)\n    original_image_patch = tf.reshape(original_image_patch, [PATCH_SIZE, PATCH_SIZE, N_BANDS])\n\n    ground_truth = tf.one_hot(ground_truth, CLASS_COUNT, dtype=tf.float32)\n    ground_truth = tf.reshape(ground_truth, [PATCH_SIZE, PATCH_SIZE, CLASS_COUNT])\n\n    return original_image_patch, ground_truth\n\n\ndef parse_example_factory_with_augmentation(crop_boxes):\n    global zeros, ones\n    zeros = tf.zeros(GROUND_TRUTH_SHAPE, dtype=tf.float32)\n    ones = tf.ones(GROUND_TRUTH_SHAPE, dtype=tf.float32)\n\n    def inner_parse_example(example_proto):\n        patch_img, patch_mask = parse_example(example_proto)\n        return augment(patch_img, patch_mask, crop_boxes)\n\n    return inner_parse_example\n\n\ndef parse_example_factory():\n    global zeros, ones\n    zeros = tf.zeros(GROUND_TRUTH_SHAPE, dtype=tf.float32)\n    ones = tf.ones(GROUND_TRUTH_SHAPE, dtype=tf.float32)\n\n    def inner_parse_example(example_proto):\n        return parse_example(example_proto)\n\n    return inner_parse_example\n\n\ndef bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.numpy()]))\n\n\ndef transform_both(op, patch_img, patch_mask, args={}):\n    return op(patch_img, **args), op(patch_mask, **args)\n\n\ndef augment(patch_img, patch_mask, crop_boxes):\n    patch_img, patch_mask = random_rotate(patch_img, patch_mask)\n    patch_img, patch_mask = random_zoom_in(patch_img, patch_mask, crop_boxes)\n    return tf.squeeze(patch_img), tf.squeeze(patch_mask)\n\n\ndef random_rotate(patch_img, patch_mask):\n    random_angle = tf.random.uniform([], 0, 1, dtype=tf.float32)\n    patch_img, patch_mask = transform_both(tfa.image.rotate,  # fill mode: put zeros (masked out)\n       patch_img, patch_mask,\n       {'angles': random_angle * math.pi, 'interpolation': 'nearest'})\n    return patch_img, patch_mask\n\n\ndef random_zoom_in(patch_img, patch_mask, crop_boxes):\n    random_zoom_ind = tf.random.uniform([], 0, len(crop_boxes), dtype=tf.int32)\n    random_crop_box = [tf.constant(np.array(crop_boxes), dtype=tf.float32)[random_zoom_ind]]\n\n    patch_img = tf.image.crop_and_resize(\n        tf.expand_dims(patch_img, 0),  # needs 4D tensor\n        random_crop_box,\n        box_indices=tf.zeros(1, dtype=tf.int32),\n        crop_size=(PATCH_SIZE, PATCH_SIZE))\n    patch_mask = tf.image.crop_and_resize(\n        tf.expand_dims(patch_mask, 0),\n        random_crop_box,\n        box_indices=tf.zeros(1, dtype=tf.int32),\n        crop_size=(PATCH_SIZE, PATCH_SIZE),\n        method='nearest')  # we don't want to introduce invalid labels with interpolated values\n    return patch_img, patch_mask\n\n\n\n\n\n\n\nfrom typing import Callable, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.losses import binary_crossentropy\n\n\ndef tanimoto_loss() -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n    \"\"\"\n    Tanimoto loss. Defined in the paper \"ResUNet-a: a deep learning framework for\n    semantic segmentation of remotely sensed data\", under 3.2.4. Generalization to multiclass\n    imbalanced problems. See https://arxiv.org/pdf/1904.00592.pdf Used as loss function for\n    multi-class image segmentation with one-hot encoded masks.\n\n    :return: Tanimoto  loss function (Callable[[tf.Tensor, tf.Tensor], tf.Tensor])\n    \"\"\"\n\n    def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\"\n        Compute Tanimoto loss.\n        :param y_true: True masks (tf.Tensor,\n            shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, <N_CLASSES>))\n        :param y_pred:\n            Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>,\n        <N_CLASSES>))\n        :return: Tanimoto loss (tf.Tensor, shape=(None, ))\n        \"\"\"\n        axis_to_reduce = range(1, K.ndim(y_pred))  # All axis but first (batch)\n        numerator = y_true * y_pred\n        numerator = K.sum(numerator, axis=axis_to_reduce)\n\n        denominator = (y_true**2 + y_pred**2 - y_true * y_pred)\n        denominator = K.sum(denominator, axis=axis_to_reduce)\n        return 1 - numerator / denominator\n\n    return loss\n\n\ndef tanimoto_with_complements() -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n    \"\"\"\n    From \"ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data\"\n    \"\"\"\n    tanimoto = tanimoto_loss()\n\n    def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        normal = tanimoto(y_true, y_pred)\n        complement = tanimoto(1. - y_true, 1. - y_pred)\n        return (normal + complement) / 2\n\n    return loss\n\n\n\n\n\nfrom tensorflow.python.keras import Model\nfrom tensorflow.python.keras.callbacks import Callback\n\n\nclass SaveModelCallback(Callback):\n\n    def __init__(self, model: Model, tf_checkpoint_path: str, pickle_path: str = None,\n                 starting_val_loss=None):\n        super().__init__()\n        self.model = model\n        self.tf_checkpoint_path = tf_checkpoint_path\n        if pickle_path is not None:\n            self.pickle_path = pickle_path\n        else:\n            self.pickle_path = tf_checkpoint_path + '_pickle'\n        self.best_val_loss = starting_val_loss\n        self.save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.model.save_weights(self.tf_checkpoint_path + '_last', options=self.save_locally, )\n        if self.best_val_loss is None or logs['val_loss'] < self.best_val_loss:\n            print('saving model')\n            self.best_val_loss = logs['val_loss']\n            self.model.save_weights(self.tf_checkpoint_path, options=self.save_locally)\n            # pickle_file = open(self.pickle_path, 'wb')\n            # pickle.dump(self.model, pickle_file)\n\n            \n            \n            \n            \n            \n            \n            \n\n\n# training\n\nfrom keras_unet_collection.model_resunet_a_2d import resunet_a_2d\nimport tensorflow as tf\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \\\n    TensorBoard, CSVLogger\nfrom functools import reduce\n\nfrom geonrw_unet_model import unet_model\n\npatch_size_text = '{}'.format(PATCH_SIZE)\nresunet_a_weights_path = 'weights_resunet_a_p' + patch_size_text\nunet_weights_path = 'weights_unet_p' + patch_size_text\nif not os.path.exists(resunet_a_weights_path):\n    os.makedirs(resunet_a_weights_path)\nif not os.path.exists(unet_weights_path):\n    os.makedirs(unet_weights_path)\nresunet_a_weights_path += '/saved_model'\nunet_weights_path += '/saved_model'\n\nresunet_a_saved_weights_path = RESUNETA_SAVED_PATH + '/weights_resunet_a_p256/saved_model'\nunet_saved_weights_path = ''\n\n\ndef compile_model(model):\n    # tf.python.keras.optimizer_v2.adam.Adam(learning_rate=x)   or 'adam' if that doesn't work\n    model.compile(optimizer='adam', loss=tanimoto_with_complements())\n    return model\n\n\ndef get_model_unet():\n    model = unet_model(CLASS_COUNT, PATCH_SIZE, n_channels=N_BANDS, upconv=True)\n    compile_model(model)\n    return model\n\n\ndef get_model_resunet_a(batch_size):\n    batch_norm = batch_size >= 2\n    model = resunet_a_2d((PATCH_SIZE, PATCH_SIZE, N_BANDS), [32, 64, 128, 256, 512, 1024],\n                         dilation_num=[1, 3, 15, 31],\n                         n_labels=CLASS_COUNT,\n                         activation='ReLU', output_activation='Sigmoid',\n                         batch_norm=batch_norm, unpool=True, name='resunet')\n    compile_model(model)\n    return model\n\n\ndef make_crop_boxes_for_random_zoom_in():\n    scales = list(np.arange(0.52, 1.0, 0.04))\n    variation_count = 3\n    crop_boxes = []  # will contain not centered boxes too\n    for scale in scales:\n        for j in range(variation_count):\n            for k in range(variation_count):\n                zoom_in_amount = 1. - scale\n                x1 = 0. + j * (zoom_in_amount / 2.)\n                y1 = 0. + k * (zoom_in_amount / 2.)\n                x2 = x1 + scale\n                y2 = y1 + scale\n                crop_boxes.append([x1, y1, x2, y2])\n    return crop_boxes\n\n\ndef count_available_inputs(tfrecord_paths: [str]):\n    add_inputs_count_from_path = lambda i, path: i + int(path.split('_')[1])\n    return reduce(add_inputs_count_from_path, tfrecord_paths, 0)\n\n\ndef setup_input_stream(\n    usage_kind: str,\n    batch_size=BASIC_BATCH_SIZE,\n    data_divider=DATA_DIVIDER\n):\n    input_path = INPUT_PATH.format(usage_kind)\n    \n    tfrecord_paths = tf.io.gfile.glob(input_path + '/*.tfrec')\n    inputs_used = int(count_available_inputs(tfrecord_paths) / DATA_DIVIDER)\n    print('{} inputs used: {}'.format(usage_kind, inputs_used))\n\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    if usage_kind == 'train':\n        crop_boxes = make_crop_boxes_for_random_zoom_in()\n        parser = parse_example_factory_with_augmentation(crop_boxes)\n    else:\n        parser = parse_example_factory()\n    dataset = tf.data.TFRecordDataset(tfrecord_paths, num_parallel_reads=tf.data.AUTOTUNE)\\\n        .with_options(ignore_order)\\\n        .take(inputs_used)\\\n        .map(parser, num_parallel_calls=tf.data.AUTOTUNE)\\\n        .repeat()\\\n        .batch(batch_size, drop_remainder=True)\\\n        .prefetch(buffer_size=tf.data.AUTOTUNE)\n    if usage_kind == 'train':\n        dataset = dataset.shuffle(buffer_size=50, reshuffle_each_iteration=True)\n    return dataset, inputs_used\n\n\ndef train_net(batch_size, is_unet):\n    if is_unet:\n        weights_path = unet_weights_path\n        saved_weights_path = unet_saved_weights_path\n        batch_size = batch_size * UNET_BATCH_MULTIPLIER\n        model = get_model_unet(batch_size)\n        model_title = 'U-Net'\n    else:  # ResUNet-a\n        weights_path = resunet_a_weights_path\n        saved_weights_path = resunet_a_saved_weights_path\n        model = get_model_resunet_a(batch_size)\n        model_title = 'ResUNet-a'\n        \n    train_inputs, train_inputs_used = setup_input_stream('train', batch_size)\n    val_inputs, val_inputs_used = setup_input_stream('val', batch_size)\n\n    train_steps_per_epoch = int(train_inputs_used / batch_size)\n    print(\"Starting training {}, batch size: {}, training steps count: {}\"\n          .format(model_title, batch_size, train_steps_per_epoch))\n\n    if CONTINUE_LEARNING:\n        print('LOADING WEIGHTS')\n        model.load_weights(saved_weights_path + 'last')\n\n    callbacks = assemble_callbacks(weights_path, model)\n\n    model.fit(train_inputs,\n              batch_size=batch_size,\n              epochs=N_EPOCHS,\n              verbose=1,\n              shuffle=True,\n              callbacks=callbacks,\n              validation_data=val_inputs,\n              steps_per_epoch=train_steps_per_epoch,\n              validation_steps=int(val_inputs_used / batch_size),\n              )\n    return model\n\n\ndef assemble_callbacks(weights_path, model):\n    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0,\n                                   patience=EARLY_STOPPING_PATIENCE, verbose=1, mode='auto')\n    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.33, patience=5)\n    model_checkpoint = SaveModelCallback(model, weights_path)\n    csv_logger = CSVLogger('log_unet.csv', append=True, separator=';')\n    callbacks = [model_checkpoint, csv_logger, reduce_lr, early_stopping]\n    return callbacks\n\n\ndef run_train():\n    try:  # detect TPUs\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n        strategy = tf.distribute.TPUStrategy(tpu)\n        batch_size = BASIC_BATCH_SIZE * TPU_BATCH_SIZE_MULTIPLIER\n    except ValueError:  # detect GPUs\n        strategy = tf.distribute.get_strategy()  # default strategy that works on CPU and single GPU\n        batch_size = BASIC_BATCH_SIZE\n\n    with strategy.scope():\n        train_net(batch_size, is_unet=False)\n        #train_net(batch_size, is_unet=True)","metadata":{"_uuid":"da3932e7-b8f8-4252-ad8a-e1fc706dcfa7","_cell_guid":"87e5aca0-5469-45d9-a230-643a8e66cb3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T04:55:57.714697Z","iopub.execute_input":"2021-09-14T04:55:57.715016Z","iopub.status.idle":"2021-09-14T04:55:57.783126Z","shell.execute_reply.started":"2021-09-14T04:55:57.714988Z","shell.execute_reply":"2021-09-14T04:55:57.782309Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"run_train()","metadata":{"_uuid":"f77578e4-68f8-4aba-a30e-3d9c1409ca33","_cell_guid":"ae70fd98-fe79-4111-b214-3b6ce7d367e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T04:55:59.133346Z","iopub.execute_input":"2021-09-14T04:55:59.133711Z","iopub.status.idle":"2021-09-14T05:12:42.728531Z","shell.execute_reply.started":"2021-09-14T04:55:59.133680Z","shell.execute_reply":"2021-09-14T05:12:42.726449Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Received dilation rates: [1, 3, 15, 31]\nExpanding dilation rates:\n\tdepth-0, dilation_rate = [1, 3, 15, 31]\n\tdepth-1, dilation_rate = [1, 3, 15, 31]\n\tdepth-2, dilation_rate = [1, 3, 15]\n\tdepth-3, dilation_rate = [1, 3, 15]\n\tdepth-4, dilation_rate = [1]\n\tdepth-5, dilation_rate = [1]\ntrain inputs used: 54114\nval inputs used: 5472\nStarting training ResUNet-a, batch size: 2, training steps count: 27057\nLOADING WEIGHTS\nEpoch 1/250\n  402/27057 [..............................] - ETA: 2:50:17 - loss: 0.3365","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-68274d7e071c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-a0cd15d0021b>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_unet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;31m#train_net(batch_size, is_unet=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-a0cd15d0021b>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(batch_size, is_unet)\u001b[0m\n\u001b[1;32m    506\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_inputs_used\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m               )\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tensorflow.python.keras import backend as K\n\nUSE_UNET = False\nRUN_ON_CPU = False\nEVALUATION_BATCH_SIZE = int(128 / BASIC_BATCH_SIZE)\nif USE_UNET:\n    EVALUATION_BATCH_SIZE *= UNET_BATCH_MULTIPLIER\n\n\nclass Evaluator:\n    def __init__(self, is_unet=True):\n        if is_unet:\n            self.model = get_model_unet()\n            self.model.load_weights(unet_weights_path)\n            self.batch_size = BASIC_BATCH_SIZE * UNET_BATCH_MULTIPLIER\n        else:\n            self.model = get_model_resunet_a(BASIC_BATCH_SIZE)\n            self.model.load_weights(resunet_a_saved_weights_path)\n            self.batch_size = BASIC_BATCH_SIZE\n        print('loading finished')\n        self.test_sequence = setup_input_stream('test', self.batch_size).as_numpy_iterator()\n        self.list_of_masked_ground_truth_categories = []\n        self.masked_predictions = []\n        self.confusion_matrices = []\n        self.accuracies = []\n        self.test_iteration_count = \\\n            int(MAX_VAL_STEPS_PER_EPOCH / EVALUATION_BATCH_SIZE / DATA_DIVIDER)\n\n    def run(self):\n        # go through the test_sequence and compute the metrics\n        # computing confusion matrix requires a lot of RAM, that is why we need to do a number of\n        #    iterations of the main loop instead of just one loop. If we would have the\n        #    EVALUATION_BATCH_SIZE a small number it could happen that there would be some\n        #    category of which we would not see in any test image, and it would ruin our final\n        #    confusion matrix, it would not have percentages as we'd like, because when a category\n        #    is not seen in an iteration, it would have 0s in its row in the conf matrix.\n        for k in range(self.test_iteration_count):\n            for i in range(EVALUATION_BATCH_SIZE):\n                # both will have self.batch_size length of tensors\n                image, ground_truth = next(self.test_sequence)\n                # self.batch_size number of predictions\n                prediction = self.model.predict(x=image, batch_size=self.batch_size)\n                prediction = from_one_hot_to_category_indices(prediction)\n\n                ground_truth_categories = from_one_hot_to_category_indices(ground_truth)\n                ground_truth_categories_masked = \\\n                    ground_truth_categories[np.nonzero(ground_truth_categories)]\n                masked_prediction = prediction[np.nonzero(ground_truth_categories)]\n                if len(masked_prediction) == PATCH_SIZE * PATCH_SIZE * self.batch_size:\n                    self.list_of_masked_ground_truth_categories.append(\n                        ground_truth_categories_masked)\n                    self.masked_predictions.append(masked_prediction)\n\n                    self.compute_accuracy(ground_truth_categories_masked, i, masked_prediction, k)\n            self.compute_confusion_matrix()  # needs a lot of RAM!\n            self.masked_predictions = []\n            self.list_of_masked_ground_truth_categories = []\n        self.display_results()\n\n    def display_results(self):\n        average_acc = float(np.mean(self.accuracies, axis=0))\n        print('average accuracy: {:.3f}'.format(average_acc))\n        confusion_matrix = np.mean(self.confusion_matrices, axis=0)\n        labels = list(CATEGORIES.keys())\n        for i in range(CLASS_COUNT - 1):\n            print('{}: {:.3f}'.format(labels[i], float(confusion_matrix[i, i])))\n        fig = plt.figure(figsize=(10, 9))\n        ax = fig.add_subplot(1, 1, 1)\n        ConfusionMatrixDisplay(confusion_matrix, labels)\\\n            .plot(ax=ax, xticks_rotation=45., cmap='YlOrRd', values_format='.3f')\n        file_name = '/cm_{date:%Y-%m-%d_%H:%M}_avg{avg}.png'\\\n            .format(date=datetime.now(), avg=round(average_acc))\n        fig.savefig(CONFUSION_MATRIX_DIR + file_name)\n        plt.show()\n\n    def compute_confusion_matrix(self):\n        #if len(self.list_of_masked_ground_truth_categories) == EVALUATION_BATCH_SIZE:\n        confusion_matrix = metrics.confusion_matrix(\n            np.array(self.list_of_masked_ground_truth_categories).flatten(),\n            np.array(self.masked_predictions).flatten(),\n            labels=range(1, CLASS_COUNT),\n            normalize='true'\n        ) * 100\n        self.confusion_matrices.append(confusion_matrix)\n\n    def compute_accuracy(self, masked_ground_truth_categories, i, masked_prediction,\n                         test_iteration_index):\n        total_pixel_count = masked_ground_truth_categories.size\n        correct_pixel_count_unet = np.count_nonzero(\n            masked_prediction == masked_ground_truth_categories\n        )\n        accuracy = correct_pixel_count_unet / total_pixel_count * 100\n        self.accuracies.append(accuracy)\n        print('{}      ({}/{})'.format(\n            accuracy,\n            i + test_iteration_index * EVALUATION_BATCH_SIZE + 1,\n            int(self.test_iteration_count * EVALUATION_BATCH_SIZE)\n        ))\n\n        \ndef run_eval():\n    if not os.path.exists(CONFUSION_MATRIX_DIR):\n        os.makedirs(CONFUSION_MATRIX_DIR)\n    # if we are training on GPU we might not have enough RAM on it to run this too,\n    #   in this case we should use the CPU, otherwise using the GPU is faster\n    if RUN_ON_CPU:\n        config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})\n        sess = tf.compat.v1.Session(config=config)\n        with tf.Graph().as_default():\n            with sess:\n                K.set_session(sess)\n                Evaluator(is_unet=USE_UNET).run()\n    else:  # run on GPU\n        Evaluator(is_unet=USE_UNET).run()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T04:05:30.406070Z","iopub.status.idle":"2021-09-14T04:05:30.406645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_eval()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T04:05:30.408027Z","iopub.status.idle":"2021-09-14T04:05:30.408596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# maybe we'll need it to get the outputs\n#import shutil shutil.make_archive( zip_file_name , 'zip', folder_to_be_zipped)","metadata":{"_uuid":"a1706f44-494a-4df7-ad1f-f1fc33a2811d","_cell_guid":"0c7a559a-ec50-4b46-b8fe-9bdcc68e7cef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T04:05:30.409727Z","iopub.status.idle":"2021-09-14T04:05:30.410311Z"},"trusted":true},"execution_count":null,"outputs":[]}]}